================ EC2 + KINESIS AGENT + KINESIS FIREHOSE + GLUE CRAWLER + ATHENA

Provision a micro EC2 instance with default settings
sudo yum install -y aws-kinesis-agent
wget http://media.sundog-soft.com/AWSBigData/LogGenerator.zip
unzip LogGenerator.zip
chmod a+x LogGenerator.py
sudo mkdir /var/log/cadabra
cd /etc/aws-kinesis
sudo nano agent.json -> Give the source path - the path where logs will be sent / generated and the target firehose name. 
Update the firehose.endpoint if region is other than us-east-
The agent.json should look like this:

{
  "cloudwatch.emitMetrics": true,
  "kinesis.endpoint": "",
  "firehose.endpoint": "firehose.us-east-1.amazonaws.com",

  "flows": [
    {
      "filePattern": "/var/log/cadabra/*",
      "deliveryStream": "my-order-logs-kinesis-firehose"
    }
  ]
}


The Ec2 instance must be able to write to your Kinesis data firehose.
So create an appropriate role and attach it to the instance - i created kinesis-firehose-full-for-order-logs-project with full firehose access. Can restrict this further.

Created S3 bucket - my-order-logs-bucket - with default settings

I then created a kinesis data firehose - my-order-logs-kinesis-data-firehose (same name as specified in agent.json) with the S3 bucket my-order-logs-bucket as the target.
I changed the buffer interval to 60 seconds from 300 seconds.
I gave the s3 bucket prefix as main/ (for the correctly processed data) and s3 bucket error output fix as error/ (for the incorrectly processed data).

Then, going back to the EC2 instnce, started the kinesis agent using the command: sudo service aws-kinesis-agent start
Then generate the logs using the command after cd ~ (to go to home directory): 
sudo ./LogGenerator.py 250000 (note: 250000 represents the number of orders)

The data should now be available in the S3 bucket.

Note: To restart kinesis agent:
sudo service aws-kinesis-agent stop
sudo service aws-kinesis-agent start

To automatically start the kinesis agent on system restart:
sudo chkconfig aws-kinesis-agent on

============== LAMBDA


Added a lambda function - my_order_logs_lambda - with trigger as the above S3 bucket (my-order-logs-bucket) - All object create events.
This lambda will copy the files in the main landing area s3://my-order-logs-bucket/main/ to a staging area s3://my-order-logs-bucket-staging.
This is to make the redshift load - the copy command in the Glue job - simpler.
The copy command can just look into the static staging path instead of the dynamic landing path.

The IAM role must have S3 access (to read / delete objects) and the Lambda Service role for writing into CloudWatch logs.
Consider increasing the timeout parameter of lambda - the default is 3 seconds.


=============== GLUE JOB

The Glue job will copy data from staging area into redshift using a COPY command.
The Glue job will get teh credentials to connect to the redshift cluster from the Secrets Manager.
The IAM role passed in the copy command must be the same role that is associated with the Redshift cluster.
The Redshift cluster might be associated with a role that has Glue access and S3 read only access so that Redshift spectrum can read from S3.
The same role can be used in the COPY command passed from the Glue job.

The Glue job will also make an entry into DynamoDB audit table to keep track of the files processed.
The Glue job will also delete the staging area, so that Redshift does not load these files again during the next run.

============== GLUE TRIGGER

Scheduled to run every hour to start the Glue job.

=============== REDSHIFT

create schema main_data;

create table main_data.my_order_logs
(
invoiceno VARCHAR(100),
stockcode VARCHAR(100),
description VARCHAR(100),
quantity INTEGER,
invoicedate VARCHAR(100),
unitprice DECIMAL(16,2),
customerid INTEGER,
country VARCHAR(100)
)

Check this table for any errors during the Redshift load.
select * from stl_load_errors;

I was facing the "delimiter not found" issue and that turned out to be because the redshift table structure (number of columns) was not in sync with the S3 data structure.
